// workflows/settings/import-data-workflow.ts
// Import Data Workflow for KV-only architecture

import { kv } from '@/data-store/kv';
import { buildDataKey, buildIndexKey, buildLogKey } from '@/data-store/keys';
import { EntityType } from '@/types/enums';

// Centralized list of entity types for import operations
const IMPORTABLE_ENTITY_TYPES = [
  EntityType.TASK,
  EntityType.ITEM,
  EntityType.SALE,
  EntityType.FINANCIAL,
  EntityType.CHARACTER,
  EntityType.PLAYER,
  EntityType.SITE
];

export interface SettingsResult {
  success: boolean;
  message: string;
  data?: {
    results: string[];
    errors: string[];
    operation: string;
    environment: string;
    requiresClientExecution?: boolean;
    progress?: {
      current: number;
      total: number;
      percentage: number;
    };
  };
}

export interface ProgressCallback {
  (current: number, total: number, message: string): void;
}

export class ImportDataWorkflow {
  
  /**
    * Execute import data operation with timeout and progress tracking
    */
   static async execute(importData: any, progressCallback?: ProgressCallback): Promise<SettingsResult> {
     try {
       console.log('[ImportDataWorkflow] üì• Starting import data operation...');

       const isKV = Boolean(process.env.UPSTASH_REDIS_REST_URL);
       const isServer = typeof window === 'undefined';
       const results: string[] = [];
       const errors: string[] = [];

       console.log(`[ImportDataWorkflow] üåç Environment: ${isKV ? 'KV (Production)' : 'Local (Development)'}`);
       console.log(`[ImportDataWorkflow] üñ•Ô∏è Context: ${isServer ? 'Server' : 'Client'}`);
      
      // For local environment, we need to run on client side
      if (!isKV && isServer) {
        console.log(`[ImportDataWorkflow] ‚ö†Ô∏è Local environment detected on server - returning instruction for client-side execution`);
        return {
          success: true,
          message: 'Import data requires client-side execution for local environment',
          data: {
            results: ['Client-side import data required for local environment'],
            errors: [],
            operation: 'import-data',
            environment: 'local',
            requiresClientExecution: true
          }
        };
      }

      const startTime = Date.now();
      const TIMEOUT_MS = 4 * 60 * 1000; // 4 minutes (leaving 1 minute buffer for API timeout)
      const totalOperations = IMPORTABLE_ENTITY_TYPES.length + 3; // entities + settlements + links + logs

      // Validate import data
      if (!this.validateImportData(importData, errors)) {
        return {
          success: false,
          message: 'Invalid import data format',
          data: {
            results: [],
            errors,
            operation: 'import-data',
            environment: isKV ? 'kv' : 'local'
          }
        };
      }

      // Import all data with timeout and progress tracking
       try {
         await this.importAllData(importData, results, errors, progressCallback);
       } catch (error) {
         if (error instanceof Error && error.message.includes('timeout')) {
           console.error('[ImportDataWorkflow] ‚è∞ Operation timeout - attempting graceful shutdown');
           return {
             success: false,
             message: `Import operation timed out: ${error.message}`,
             data: {
               results,
               errors: [...errors, `TIMEOUT: ${error.message}`],
               operation: 'import-data',
               environment: isKV ? 'kv' : 'local',
               progress: {
                 current: Math.max(results.length, errors.length),
                 total: totalOperations,
                 percentage: Math.round((Math.max(results.length, errors.length) / totalOperations) * 100)
               }
             }
           };
         }
         throw error; // Re-throw non-timeout errors
       }
      
      const success = errors.length === 0;
      const message = success 
        ? `Successfully imported data - ${results.length} operations completed`
        : `Import data completed with ${errors.length} errors - ${results.length} operations completed`;
      
      console.log(`[ImportDataWorkflow] ‚úÖ Import data operation completed in ${Date.now() - startTime}ms: ${message}`);

      return {
        success,
        message,
        data: {
          results,
          errors,
          operation: 'import-data',
          environment: isKV ? 'kv' : 'local',
          progress: {
            current: totalOperations,
            total: totalOperations,
            percentage: 100
          }
        }
      };
      
    } catch (error) {
      console.error('[ImportDataWorkflow] ‚ùå Import data operation failed:', error);
      return {
        success: false,
        message: `Import data operation failed: ${error instanceof Error ? error.message : 'Unknown error'}`,
        data: {
          results: [],
          errors: [error instanceof Error ? error.message : 'Unknown error'],
          operation: 'import-data',
          environment: 'unknown'
        }
      };
    }
  }
  
  /**
   * Validate import data format
   */
  private static validateImportData(importData: any, errors: string[]): boolean {
    try {
      if (!importData || typeof importData !== 'object') {
        errors.push('Import data must be an object');
        return false;
      }
      
      if (!importData.entities || typeof importData.entities !== 'object') {
        errors.push('Import data must have entities property');
        return false;
      }
      
      for (const entityType of IMPORTABLE_ENTITY_TYPES) {
        if (!Array.isArray(importData.entities[entityType])) {
          errors.push(`Import data must have ${entityType} as an array`);
          return false;
        }
      }
      
      if (!Array.isArray(importData.links)) {
        errors.push('Import data must have links as an array');
        return false;
      }
      
      if (!importData.logs || typeof importData.logs !== 'object') {
        errors.push('Import data must have logs property');
        return false;
      }
      
      // Settlements are optional reference data
      if (importData.settlements && !Array.isArray(importData.settlements)) {
        errors.push('Import data settlements must be an array if present');
        return false;
      }
      
      return true;
    } catch (error) {
      errors.push(`Failed to validate import data: ${error instanceof Error ? error.message : 'Unknown error'}`);
      return false;
    }
  }
  
  /**
   * Import all data with timeout and progress tracking
   */
  private static async importAllData(importData: any, results: string[], errors: string[], progressCallback?: ProgressCallback): Promise<void> {
    try {
      console.log('[ImportDataWorkflow] üì• Importing all data...');

      const startTime = Date.now();
      const TIMEOUT_MS = 4 * 60 * 1000; // 4 minutes (leaving 1 minute buffer for API timeout)
      let importedEntityTypes = 0;
      const totalOperations = IMPORTABLE_ENTITY_TYPES.length + 3; // entities + settlements + links + logs
      let currentOperation = 0;

      // Helper function to check timeout and report progress
      const checkTimeoutAndProgress = (operation: string) => {
        currentOperation++;
        const elapsed = Date.now() - startTime;
        if (elapsed > TIMEOUT_MS) {
          throw new Error(`Operation timeout after ${elapsed}ms during: ${operation}`);
        }
        if (progressCallback) {
          const progress = Math.round((currentOperation / totalOperations) * 100);
          progressCallback(currentOperation, totalOperations, `Processing: ${operation}`);
        }
      };

      // Import all entity types
      for (const entityType of IMPORTABLE_ENTITY_TYPES) {
        try {
          checkTimeoutAndProgress(`Importing ${entityType} entities`);
          await this.importEntityType(entityType, importData.entities[entityType], results, errors);
          importedEntityTypes++;
        } catch (error) {
          if (error instanceof Error && error.message.includes('timeout')) {
            throw error; // Re-throw timeout errors
          }
          const errorMsg = `Failed to import ${entityType}: ${error instanceof Error ? error.message : 'Unknown error'}`;
          errors.push(errorMsg);
          console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
        }
      }

      // Import settlements as reference data (if present)
      if (importData.settlements && Array.isArray(importData.settlements)) {
        try {
          checkTimeoutAndProgress('Importing settlements');
          await this.importSettlements(importData.settlements, results, errors);
        } catch (error) {
          if (error instanceof Error && error.message.includes('timeout')) {
            throw error; // Re-throw timeout errors
          }
          const errorMsg = `Failed to import settlements: ${error instanceof Error ? error.message : 'Unknown error'}`;
          errors.push(errorMsg);
          console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
        }
      } else {
        currentOperation++; // Count as completed even if skipped
      }

      // Import links
      try {
        checkTimeoutAndProgress('Importing links');
        await this.importLinks(importData.links, results, errors);
      } catch (error) {
        if (error instanceof Error && error.message.includes('timeout')) {
          throw error; // Re-throw timeout errors
        }
        const errorMsg = `Failed to import links: ${error instanceof Error ? error.message : 'Unknown error'}`;
        errors.push(errorMsg);
        console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
      }

      // Import logs
      try {
        checkTimeoutAndProgress('Importing logs');
        await this.importLogs(importData.logs, results, errors);
      } catch (error) {
        if (error instanceof Error && error.message.includes('timeout')) {
          throw error; // Re-throw timeout errors
        }
        const errorMsg = `Failed to import logs: ${error instanceof Error ? error.message : 'Unknown error'}`;
        errors.push(errorMsg);
        console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
      }

      results.push(`Imported data for ${importedEntityTypes} entity types`);
      console.log(`[ImportDataWorkflow] ‚úÖ Imported all data in ${Date.now() - startTime}ms`);

    } catch (error) {
      const errorMsg = `Failed to import all data: ${error instanceof Error ? error.message : 'Unknown error'}`;
      errors.push(errorMsg);
      console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
    }
  }
  
  /**
   * Import specific entity type with batch processing
   */
  private static async importEntityType(entityType: string, entities: any[], results: string[], errors: string[]): Promise<void> {
    try {
      if (!entities || entities.length === 0) {
        results.push(`No ${entityType} entities to import`);
        return;
      }

      console.log(`[ImportDataWorkflow] üì• Processing ${entities.length} ${entityType} entities...`);

      // Clear existing entities for this type
      const indexKey = buildIndexKey(entityType);
      const existingIds = await kv.smembers(indexKey);

      if (existingIds.length > 0) {
        const dataKeys = existingIds.map(id => buildDataKey(entityType, id));
        await kv.delMany(dataKeys);
        await kv.del(indexKey);
        results.push(`Cleared ${existingIds.length} existing ${entityType} entities`);
      }

      // Filter out entities without IDs and prepare batch operations
      const validEntities = entities.filter(entity => entity && entity.id);
      const invalidCount = entities.length - validEntities.length;

      if (invalidCount > 0) {
        console.warn(`[ImportDataWorkflow] ‚ö†Ô∏è Skipped ${invalidCount} ${entityType} entities without valid IDs`);
      }

      if (validEntities.length === 0) {
        results.push(`No valid ${entityType} entities to import`);
        return;
      }

      // Batch import entities in chunks to avoid overwhelming KV
      const BATCH_SIZE = 50;
      let importedCount = 0;
      const totalBatches = Math.ceil(validEntities.length / BATCH_SIZE);

      for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
        const startIndex = batchIndex * BATCH_SIZE;
        const endIndex = Math.min(startIndex + BATCH_SIZE, validEntities.length);
        const batch = validEntities.slice(startIndex, endIndex);

        console.log(`[ImportDataWorkflow] üîÑ Processing batch ${batchIndex + 1}/${totalBatches} for ${entityType} (${batch.length} entities)`);

        try {
          // Prepare batch operations for this chunk
          const pipeline = kv.multi();

          for (const entity of batch) {
            const dataKey = buildDataKey(entityType, entity.id);
            pipeline.set(dataKey, JSON.stringify(entity));
          }

          // Add all entity IDs to index in a single operation
          const entityIds = batch.map(entity => entity.id);
          pipeline.sadd(indexKey, ...entityIds);

          // Execute batch
          await pipeline.exec();
          importedCount += batch.length;

          console.log(`[ImportDataWorkflow] ‚úÖ Batch ${batchIndex + 1}/${totalBatches} completed for ${entityType}`);
        } catch (error) {
          console.error(`[ImportDataWorkflow] ‚ùå Batch ${batchIndex + 1} failed for ${entityType}:`, error);
          // Continue with next batch instead of failing completely
        }
      }

      results.push(`Imported ${importedCount} ${entityType} entities in ${totalBatches} batches`);
      console.log(`[ImportDataWorkflow] ‚úÖ Imported ${importedCount} ${entityType} entities`);

    } catch (error) {
      const errorMsg = `Failed to import ${entityType}: ${error instanceof Error ? error.message : 'Unknown error'}`;
      errors.push(errorMsg);
      console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
    }
  }
  
  /**
   * Import links with batch processing
   */
  private static async importLinks(links: any[], results: string[], errors: string[]): Promise<void> {
    try {
      if (!links || links.length === 0) {
        results.push('No links to import');
        return;
      }

      console.log(`[ImportDataWorkflow] üîó Processing ${links.length} links...`);

      // Clear existing links
      const linksIndexKey = buildIndexKey('links');
      const existingLinkIds = await kv.smembers(linksIndexKey);

      if (existingLinkIds.length > 0) {
        const linkKeys = existingLinkIds.map(id => `links:link:${id}`);
        await kv.delMany(linkKeys);
        await kv.del(linksIndexKey);
        results.push(`Cleared ${existingLinkIds.length} existing links`);
      }

      // Filter out links without IDs
      const validLinks = links.filter(link => link && link.id);
      const invalidCount = links.length - validLinks.length;

      if (invalidCount > 0) {
        console.warn(`[ImportDataWorkflow] ‚ö†Ô∏è Skipped ${invalidCount} links without valid IDs`);
      }

      if (validLinks.length === 0) {
        results.push('No valid links to import');
        return;
      }

      // Batch import links in chunks
      const BATCH_SIZE = 50;
      let importedCount = 0;
      const totalBatches = Math.ceil(validLinks.length / BATCH_SIZE);

      for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
        const startIndex = batchIndex * BATCH_SIZE;
        const endIndex = Math.min(startIndex + BATCH_SIZE, validLinks.length);
        const batch = validLinks.slice(startIndex, endIndex);

        console.log(`[ImportDataWorkflow] üîÑ Processing links batch ${batchIndex + 1}/${totalBatches} (${batch.length} links)`);

        try {
          // Prepare batch operations for this chunk
          const pipeline = kv.multi();

          for (const link of batch) {
            const linkKey = `links:link:${link.id}`;
            pipeline.set(linkKey, JSON.stringify(link));
          }

          // Add all link IDs to index in a single operation
          const linkIds = batch.map(link => link.id);
          pipeline.sadd(linksIndexKey, ...linkIds);

          // Execute batch
          await pipeline.exec();
          importedCount += batch.length;

          console.log(`[ImportDataWorkflow] ‚úÖ Links batch ${batchIndex + 1}/${totalBatches} completed`);
        } catch (error) {
          console.error(`[ImportDataWorkflow] ‚ùå Links batch ${batchIndex + 1} failed:`, error);
          // Continue with next batch instead of failing completely
        }
      }

      results.push(`Imported ${importedCount} links in ${totalBatches} batches`);
      console.log(`[ImportDataWorkflow] ‚úÖ Imported ${importedCount} links`);

    } catch (error) {
      const errorMsg = `Failed to import links: ${error instanceof Error ? error.message : 'Unknown error'}`;
      errors.push(errorMsg);
      console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
    }
  }
  
  /**
   * Import logs with batch processing
   */
  private static async importLogs(logs: any, results: string[], errors: string[]): Promise<void> {
    try {
      const logTypes = [...IMPORTABLE_ENTITY_TYPES, 'links']; // links is special case

      for (const logType of logTypes) {
        try {
          const logEntries = logs[logType] || [];

          if (logEntries.length === 0) {
            results.push(`No ${logType} log entries to import`);
            continue;
          }

          console.log(`[ImportDataWorkflow] üìù Processing ${logEntries.length} ${logType} log entries...`);

          // Clear existing logs for this type
          const logKey = buildLogKey(logType);
          await kv.del(logKey);

          // Batch import log entries in chunks (logs can be very large)
          const BATCH_SIZE = 100;
          let importedCount = 0;
          const totalBatches = Math.ceil(logEntries.length / BATCH_SIZE);

          for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
            const startIndex = batchIndex * BATCH_SIZE;
            const endIndex = Math.min(startIndex + BATCH_SIZE, logEntries.length);
            const batch = logEntries.slice(startIndex, endIndex);

            console.log(`[ImportDataWorkflow] üîÑ Processing logs batch ${batchIndex + 1}/${totalBatches} for ${logType} (${batch.length} entries)`);

            try {
              // For logs, we need to use individual lpush operations since Redis LPUSH doesn't support batch operations easily
              // But we can still process them in reasonable chunks
              for (const logEntry of batch) {
                try {
                  await kv.lpush(logKey, JSON.stringify(logEntry));
                  importedCount++;
                } catch (error) {
                  console.warn(`[ImportDataWorkflow] ‚ö†Ô∏è Failed to import ${logType} log entry:`, error);
                }
              }

              console.log(`[ImportDataWorkflow] ‚úÖ Logs batch ${batchIndex + 1}/${totalBatches} completed for ${logType}`);
            } catch (error) {
              console.error(`[ImportDataWorkflow] ‚ùå Logs batch ${batchIndex + 1} failed for ${logType}:`, error);
              // Continue with next batch instead of failing completely
            }
          }

          results.push(`Imported ${importedCount} ${logType} log entries in ${totalBatches} batches`);
        } catch (error) {
          console.warn(`[ImportDataWorkflow] ‚ö†Ô∏è Failed to import ${logType} logs:`, error);
        }
      }

      console.log(`[ImportDataWorkflow] ‚úÖ Imported logs for ${logTypes.length} log types`);

    } catch (error) {
      const errorMsg = `Failed to import logs: ${error instanceof Error ? error.message : 'Unknown error'}`;
      errors.push(errorMsg);
      console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
    }
  }

  /**
   * Import settlements as reference data with batch processing
   */
  private static async importSettlements(settlements: any[], results: string[], errors: string[]): Promise<void> {
    try {
      if (!settlements || settlements.length === 0) {
        results.push('No settlements to import');
        return;
      }

      console.log(`[ImportDataWorkflow] üèòÔ∏è Processing ${settlements.length} settlements...`);

      // Clear existing settlements
      const settlementsIndexKey = buildIndexKey('settlements');
      const existingSettlementIds = await kv.smembers(settlementsIndexKey);

      if (existingSettlementIds.length > 0) {
        const dataKeys = existingSettlementIds.map(id => buildDataKey('settlements', id));
        await kv.delMany(dataKeys);
        await kv.del(settlementsIndexKey);
        results.push(`Cleared ${existingSettlementIds.length} existing settlements`);
      }

      // Filter out settlements without IDs
      const validSettlements = settlements.filter(settlement => settlement && settlement.id);
      const invalidCount = settlements.length - validSettlements.length;

      if (invalidCount > 0) {
        console.warn(`[ImportDataWorkflow] ‚ö†Ô∏è Skipped ${invalidCount} settlements without valid IDs`);
      }

      if (validSettlements.length === 0) {
        results.push('No valid settlements to import');
        return;
      }

      // Batch import settlements in chunks
      const BATCH_SIZE = 50;
      let importedCount = 0;
      const totalBatches = Math.ceil(validSettlements.length / BATCH_SIZE);

      for (let batchIndex = 0; batchIndex < totalBatches; batchIndex++) {
        const startIndex = batchIndex * BATCH_SIZE;
        const endIndex = Math.min(startIndex + BATCH_SIZE, validSettlements.length);
        const batch = validSettlements.slice(startIndex, endIndex);

        console.log(`[ImportDataWorkflow] üîÑ Processing settlements batch ${batchIndex + 1}/${totalBatches} (${batch.length} settlements)`);

        try {
          // Prepare batch operations for this chunk
          const pipeline = kv.multi();

          for (const settlement of batch) {
            const dataKey = buildDataKey('settlements', settlement.id);
            pipeline.set(dataKey, JSON.stringify(settlement));
          }

          // Add all settlement IDs to index in a single operation
          const settlementIds = batch.map(settlement => settlement.id);
          pipeline.sadd(settlementsIndexKey, ...settlementIds);

          // Execute batch
          await pipeline.exec();
          importedCount += batch.length;

          console.log(`[ImportDataWorkflow] ‚úÖ Settlements batch ${batchIndex + 1}/${totalBatches} completed`);
        } catch (error) {
          console.error(`[ImportDataWorkflow] ‚ùå Settlements batch ${batchIndex + 1} failed:`, error);
          // Continue with next batch instead of failing completely
        }
      }

      results.push(`Imported ${importedCount} settlements as reference data in ${totalBatches} batches`);
      console.log(`[ImportDataWorkflow] ‚úÖ Imported ${importedCount} settlements`);

    } catch (error) {
      const errorMsg = `Failed to import settlements: ${error instanceof Error ? error.message : 'Unknown error'}`;
      errors.push(errorMsg);
      console.error(`[ImportDataWorkflow] ‚ùå ${errorMsg}`);
    }
  }
}
